{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the updated code for v8.5.0\n",
    "# this is a code that fit all basins\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "# print(os.getcwd())\n",
    "# os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\My Drive\\Veld_Code\\EF\\v811\\Raw\n"
     ]
    }
   ],
   "source": [
    "# set the working folder before start\n",
    "%cd \"G:\\My Drive\\Veld_Code\\EF\\v811\\Raw\"\n",
    "\n",
    "# Assign Basin here, which are: WIL, APP, EF, PER, HAY, BAR, DJ, MC\n",
    "BASIN = 'EF'\n",
    "\n",
    "# if we use ENERTEL allocated production data or not (for TX and LA)\n",
    "# ENERTEL = 'NO'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# this is to get distance in feet\n",
    "# 1 km = 3280.84 ft, the last part return will be km times 3280.84\n",
    "\n",
    "def get_distance(lat_1, lng_1, lat_2, lng_2): \n",
    "    lng_1, lat_1, lng_2, lat_2 = map(math.radians, [lng_1, lat_1, lng_2, lat_2])\n",
    "    d_lat = lat_2 - lat_1\n",
    "    d_lng = lng_2 - lng_1 \n",
    "\n",
    "    temp = (  \n",
    "         math.sin(d_lat / 2) ** 2 \n",
    "       + math.cos(lat_1) \n",
    "       * math.cos(lat_2) \n",
    "       * math.sin(d_lng / 2) ** 2\n",
    "    )\n",
    "\n",
    "    return 6373.0 * 3280.84 * (2 * math.atan2(math.sqrt(temp), math.sqrt(1 - temp)))\n",
    "\n",
    "# get_distance(52.2296756,21.0122287,52.406374,16.9251681)\n",
    "# should be 278.546 km * 3280.84 ft/km = 913,864.85864 ft\n",
    "\n",
    "\n",
    "# header_needed_columns = ['API','MeasuredDepth',\n",
    "#        'TrueVerticalDepth', 'LateralLength', \n",
    "#         'BHLatitude', 'BHLongitude','Latitude', 'Longitude','CurrentOperator', 'OriginalOperator',\n",
    "#         'WellName','WellType', 'WellStatus',\n",
    "#        'Field', 'County', 'State',\n",
    "#        'Country', 'WellBoreProfile','PermitDate',\n",
    "#        'SpudDate', 'CompletionDate', 'Basin', 'Play', 'Lease', 'LeaseId', 'GroundElevation',\n",
    "#        'PrimaryFormation','ReportedCurrentOperator','ReportedOriginalOperator','FirstProdDate','LastProdDate']\n",
    "\n",
    "header_needed_columns = ['API','MeasuredDepth',\n",
    "       'TrueVerticalDepth', 'LateralLength', 'Latitude', 'Longitude','Heel_Lat','Heel_Long',\n",
    "        'BHLatitude', 'BHLongitude','CurrentOperator', 'OriginalOperator',\n",
    "        'WellName','WellType', 'WellStatus',\n",
    "       'Field', 'County', 'State',\n",
    "       'Country', 'WellBoreProfile','PermitDate',\n",
    "       'SpudDate', 'CompletionDate', 'Basin', 'Play', 'Lease', 'LeaseId', 'GroundElevation',\n",
    "       'PrimaryFormation','ReportedCurrentOperator','ReportedOriginalOperator','FirstProdDate', 'LastProdDate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I: Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw file:\n",
      "(27645, 69)\n",
      "****************************************************************************************************\n",
      "raw data total:\n",
      "(27645, 34)\n",
      "****************************************************************************************************\n",
      "golden data total:\n",
      "(27786, 11)\n",
      "****************************************************************************************************\n",
      "raw data that matched golden database API:\n",
      "(27562, 34)\n",
      "****************************************************************************************************\n",
      "the well API that are not in golden database and set aside:\n",
      "(83, 34)\n",
      "****************************************************************************************************\n",
      "merged raw files with golden database after removing vertical wells etc:\n",
      "(27628, 34)\n"
     ]
    }
   ],
   "source": [
    "# function 0.1: merge the new data with our golden database\n",
    "# then fiture out what data are still missing\n",
    "\n",
    "def Merge_w_Previous(df_new, df_golden):\n",
    "    \n",
    "    \n",
    "    print('raw file:')\n",
    "    print(df_new.shape)\n",
    "    print('*'*100)\n",
    "    \n",
    "    df_new = df_new[header_needed_columns]\n",
    "    df_new['API_10'] = df_new['API'] / 10000\n",
    "    df_new['API_10'] = df_new['API_10'].astype(np.int64)\n",
    "    df_new['API'] = df_new['API_10'] * 10000\n",
    "    df_new['API'] = df_new['API'].astype(np.int64)\n",
    "    df_new = df_new[df_new['API'] > 0]\n",
    "    df_new.drop_duplicates(subset=['API'],keep='last',inplace=True)\n",
    "    df_new = df_new.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    df_match_golden = df_new[df_new['API'].isin(df_golden['API'])]\n",
    "    df_set_aside = df_new[~df_new['API'].isin(df_golden['API'])]\n",
    "    \n",
    "    print('raw data total:')\n",
    "    print(df_new.shape)\n",
    "    print('*'*100)\n",
    "    print('golden data total:')\n",
    "    print(df_golden.shape)\n",
    "    print('*'*100)\n",
    "    print('raw data that matched golden database API:')\n",
    "    print(df_match_golden.shape)\n",
    "    print('*'*100)\n",
    "    print('the well API that are not in golden database and set aside:')\n",
    "    print(df_set_aside.shape)    \n",
    "    \n",
    "    df_match_golden = df_match_golden.drop(['MeasuredDepth','TrueVerticalDepth','LateralLength','Latitude','Longitude',\n",
    "                                              'Heel_Lat','Heel_Long','BHLatitude','BHLongitude'], axis=1)\n",
    "    df_match_golden = pd.merge(df_match_golden,df_golden[['API','MeasuredDepth','TrueVerticalDepth','LateralLength',\n",
    "                                              'Latitude','Longitude','Heel_Lat','Heel_Long','BHLatitude','BHLongitude'\n",
    "                                             ]],on='API', how='inner')\n",
    "\n",
    "    df_match_golden = df_match_golden[header_needed_columns]\n",
    "    \n",
    "    df_new_merge = pd.concat([df_match_golden,df_set_aside])\n",
    "    \n",
    "    df_new_merge = df_new_merge[df_new_merge['MeasuredDepth'] != 'remove']\n",
    "    \n",
    "    print('*'*100)\n",
    "    print('merged raw files with golden database after removing vertical wells etc:')\n",
    "    print(df_new_merge.shape)      \n",
    "    \n",
    "    return df_new_merge\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_raw_header_00 = pd.read_csv('0000_Raw_Header_Final_Step_0.csv') \n",
    "df_golden = pd.read_excel('0000_Golden_Missing_Database_20211130.xlsx')\n",
    "\n",
    "\n",
    "# Merge_w_Previous(df_raw_header_00, df_golden)\n",
    "\n",
    "df_raw_header_01 = Merge_w_Previous(df_raw_header_00, df_golden)\n",
    "df_raw_header_01.to_csv('df_raw_header_v811_merge_w_previous.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 34)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# function 0.2: just using the Raw_Header to get the missing TVD, LL, MD, SHL, BHL for Rollyn\n",
    "# this function is for unconventional wells only\n",
    "\n",
    "def Get_Missing_Data(df_header):\n",
    "    \n",
    "    \n",
    "    # get the distance between SHL and BHL\n",
    "    for i in range(df_header.shape[0]):\n",
    "        df_header.loc[i,'SL_BH_Distance'] = get_distance(df_header.loc[i,'Latitude'],\n",
    "                                                         df_header.loc[i,'Longitude'],\n",
    "                                                         df_header.loc[i,'BHLatitude'],\n",
    "                                                         df_header.loc[i,'BHLongitude']\n",
    "        )\n",
    "        \n",
    "    # get the suspicious wells with SHL to BHL distance more than 21120' or less than 500'\n",
    "    df_header_bad_sh_bh_distance = df_header[ (df_header['SL_BH_Distance'] > 21120) \n",
    "#                                              |\n",
    "#                                               (df_header['SL_BH_Distance'] < 10)\n",
    "                                            ] \n",
    "        \n",
    "        \n",
    "        \n",
    "    # get missing TVD\n",
    "    df_header_tvd = df_header[df_header['TrueVerticalDepth'] > 0]\n",
    "    df_header_tvd = df_header_tvd[df_header_tvd['MeasuredDepth'] != df_header_tvd['TrueVerticalDepth']]\n",
    "    df_header_no_TVD = df_header[~df_header.API.isin(df_header_tvd.API)]\n",
    "    \n",
    "    # get missing LL\n",
    "    df_header_LL = df_header[df_header['LateralLength'] > 1000]\n",
    "    df_header_LL = df_header_LL[df_header_LL['LateralLength'] < 25000]\n",
    "    df_header_no_LL = df_header[~df_header.API.isin(df_header_LL.API)]\n",
    "    \n",
    "    # get missing operator\n",
    "    df_header_no_Operator = df_header[df_header['CurrentOperator'].isnull()]\n",
    "    \n",
    "    # get the missing SHL\n",
    "    df_header_no_SHL = df_header[df_header['Latitude'].isnull()]\n",
    "    \n",
    "    # get the missing BHL\n",
    "    df_header_no_BHL = df_header[df_header['BHLatitude'].isnull()]\n",
    "    \n",
    "    \n",
    "    df_missing_well_data = pd.concat([df_header_no_TVD,\n",
    "                                      df_header_no_LL,\n",
    "                                      df_header_no_Operator,\n",
    "                                      df_header_bad_sh_bh_distance,\n",
    "                                      df_header_no_SHL,\n",
    "                                      df_header_no_BHL\n",
    "                                     ])\n",
    "    \n",
    "    # drop the duplicates for Rollyn\n",
    "    df_missing_well_data.drop_duplicates(subset='API', inplace = True)\n",
    "    \n",
    "    \n",
    "    return df_missing_well_data\n",
    "\n",
    "\n",
    "Raw_Header_0 = pd.read_csv('df_raw_header_v811_merge_w_previous.csv')\n",
    "\n",
    "\n",
    "\n",
    "Raw_Header_0 = Raw_Header_0[header_needed_columns]\n",
    "df_missing_well_data = Get_Missing_Data(Raw_Header_0)\n",
    "df_missing_well_data.to_excel(\"df_missing_well_data.xlsx\",index=False)\n",
    "df_missing_well_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27619, 34)\n",
      "27619\n",
      "(22, 33)\n",
      "22\n",
      "(27641, 34)\n",
      "27641\n",
      "****************\n",
      "(2230817, 28)\n",
      "27626\n",
      "(274, 28)\n",
      "7\n",
      "(2231091, 28)\n",
      "27633\n",
      "****************\n"
     ]
    }
   ],
   "source": [
    "# function 1: merge 2 seperate data if needed, and re-export as raw data standard\n",
    "# use case is to add the private data to exisiting public database\n",
    "\n",
    "Raw_Header_1 = pd.read_csv('df_raw_header_v811_merge_w_previous.csv')\n",
    "Raw_Header_2 = pd.read_csv('EF_v811_Addon_20220305_22_Wells-20220932050-dR0C6-Header.csv')\n",
    "Raw_Header_2 = Raw_Header_2[header_needed_columns]\n",
    "\n",
    "Raw_Monthly_Production_1 = pd.read_csv('EF_v811_Base_20220305-20220632031-1BD8P-Production.csv')\n",
    "Raw_Monthly_Production_2 = pd.read_csv('EF_v811_Addon_20220305_22_Wells-20220932050-dR0C6-Production.csv')\n",
    "\n",
    "# Raw_Completion_1 = pd.read_csv('11.29.2021_Permian_30239_Wells-202129111735-Hw2YK-Stimulation.csv')\n",
    "# Raw_Completion_2 = pd.read_csv('11.29.2021_Permian_3328_Permits_2Yr-202129111722-6g04r-Stimulation.csv')\n",
    "\n",
    "\n",
    "Raw_Header = pd.concat([Raw_Header_1,Raw_Header_2])\n",
    "Raw_Monthly_Production = pd.concat([Raw_Monthly_Production_1,Raw_Monthly_Production_2])\n",
    "# Raw_Completion = pd.concat([Raw_Completion_1,Raw_Completion_2])\n",
    "\n",
    "Raw_Header.to_csv('0000_Raw_Header_Final.csv',index=False)\n",
    "Raw_Monthly_Production.to_csv('0000_Raw_Monthly_Production.csv',index=False)\n",
    "# Raw_Completion.to_csv('0000_Raw_Completion.csv')\n",
    "\n",
    "\n",
    "print(Raw_Header_1.shape)\n",
    "print(Raw_Header_1.API.nunique())\n",
    "print(Raw_Header_2.shape)\n",
    "print(Raw_Header_2.API.nunique())\n",
    "print(Raw_Header.shape)\n",
    "print(Raw_Header.API.nunique())\n",
    "\n",
    "\n",
    "print(\"****************\")\n",
    "\n",
    "print(Raw_Monthly_Production_1.shape)\n",
    "print(Raw_Monthly_Production_1.API.nunique())\n",
    "print(Raw_Monthly_Production_2.shape)\n",
    "print(Raw_Monthly_Production_2.API.nunique())\n",
    "print(Raw_Monthly_Production.shape)\n",
    "print(Raw_Monthly_Production.API.nunique())\n",
    "\n",
    "\n",
    "print(\"****************\")\n",
    "\n",
    "# print(Raw_Completion_1.shape)\n",
    "# print(Raw_Completion_1.API.nunique())\n",
    "# print(Raw_Completion_2.shape)\n",
    "# print(Raw_Completion_2.API.nunique())\n",
    "# print(Raw_Completion.shape)\n",
    "# print(Raw_Completion.API.nunique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before running start, rename all input files to:\n",
    "# \"0000_Raw_Header.csv\"\n",
    "# \"0000_Raw_Monthly_Production.csv\"\n",
    "# \"0000_Raw_Completion.csv\"\n",
    "# \"0000_Raw_Spacing.csv\"\n",
    "\n",
    "# Import raw header, completion, production summary and monthly production data from WellDatabase\n",
    "Raw_Header                =    pd.read_csv('0000_Raw_Header_Final.csv')\n",
    "Raw_Monthly_Production    =    pd.read_csv('0000_Raw_Monthly_Production.csv')\n",
    "Raw_Completion            =    pd.read_csv('0000_Raw_Completion.csv')\n",
    "# Raw_Spacing               =    pd.read_csv('0000_Raw_Spacing.csv')\n",
    "\n",
    "\n",
    "Raw_Header['API_10'] = Raw_Header['API'] / 10000\n",
    "Raw_Header['API_10'] = Raw_Header['API_10'].astype(np.int64)\n",
    "Raw_Header['API'] = Raw_Header['API_10'] * 10000\n",
    "Raw_Header['API'] = Raw_Header['API'].astype(np.int64)\n",
    "Raw_Header.drop_duplicates(subset=['API'],keep='last',inplace=True)\n",
    "Raw_Header = Raw_Header.reset_index(drop=True)\n",
    "\n",
    "# export the raw header for future use \n",
    "# mainly to generate the disqualified list and pie chart, otherwise it will cause bug\n",
    "Raw_Header.to_csv('0000_Raw_Header_Final.csv',index=False)\n",
    "\n",
    "\n",
    "Raw_Monthly_Production.dropna(subset=['API'],inplace=True)\n",
    "Raw_Monthly_Production['API'] = Raw_Monthly_Production['API'].astype(np.int64)\n",
    "Raw_Completion.dropna(subset=['API'],inplace=True)\n",
    "Raw_Completion['API'] = Raw_Completion['API'].astype(np.int64)\n",
    "\n",
    "\n",
    "# use the filtered column\n",
    "Raw_Header = Raw_Header[header_needed_columns]\n",
    "\n",
    "# fill all the Primary Formation and Baisn Column\n",
    "Raw_Header['PrimaryFormation'].fillna('UNKNOWN', inplace=True)\n",
    "\n",
    "if BASIN == 'WIL':\n",
    "    Raw_Header['Basin'] = 'WILLISTON'\n",
    "\n",
    "if BASIN == 'APP':\n",
    "    Raw_Header['Basin'] = 'APPALACHIAN'\n",
    "    \n",
    "if BASIN == 'EF':\n",
    "    Raw_Header['Basin'] = 'EAGLE FORD'\n",
    "\n",
    "if BASIN == 'HAY':\n",
    "    Raw_Header['Basin'] = 'HAYNESVILLE'\n",
    "\n",
    "if BASIN == 'DJ':\n",
    "    Raw_Header['Basin'] = 'DJ'\n",
    "    \n",
    "if BASIN == 'BAR':\n",
    "    Raw_Header['Basin'] = 'BARNETT'\n",
    "\n",
    "if BASIN == 'PER':\n",
    "    Raw_Header['Basin'] = 'PERMIAN'\n",
    "\n",
    "if BASIN == 'MC':\n",
    "    Raw_Header['Basin'] = 'MIDCONTINENT'\n",
    "\n",
    "        \n",
    "\n",
    "# fix the edge cases where SHL is null and BHL has values\n",
    "Raw_Header.Latitude = np.where(Raw_Header.Latitude.isnull(), Raw_Header.BHLatitude, Raw_Header.Latitude)\n",
    "Raw_Header.Longitude = np.where(Raw_Header.Longitude.isnull(), Raw_Header.BHLongitude, Raw_Header.Longitude)\n",
    "\n",
    "# fix the Current Operator blanks automatically \n",
    "Raw_Header.CurrentOperator = np.where(Raw_Header.CurrentOperator.isnull(), \n",
    "                                      Raw_Header.OriginalOperator, Raw_Header.CurrentOperator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27641, 33)\n",
      "27641\n",
      "********************\n",
      "(2576773, 10)\n",
      "27634\n",
      "********************\n",
      "(39767, 62)\n",
      "24529\n"
     ]
    }
   ],
   "source": [
    "print(Raw_Header.shape)\n",
    "print(Raw_Header.API.nunique())\n",
    "print('*'*20)\n",
    "print(Raw_Monthly_Production.shape)\n",
    "print(Raw_Monthly_Production.API.nunique())\n",
    "print('*'*20)\n",
    "print(Raw_Completion.shape)\n",
    "print(Raw_Completion.API.nunique())\n",
    "# print(Raw_Spacing.shape)\n",
    "# print(Raw_Spacing.API.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "min API in header           =  14\n",
      "min API in QMM              =  14\n",
      "min API in Completion       =  10\n",
      "********************\n",
      "after, check min and max, they have to be equal in order to move forward\n",
      "min API in header           =  14\n",
      "min API in QMM              =  14\n",
      "min API in Completion       =  14\n",
      "********************\n",
      "max API in header           =  14\n",
      "max API in QMM              =  14\n",
      "max API in Completion       =  14\n"
     ]
    }
   ],
   "source": [
    "# function 2: check the API of all header, production, completion and spacing\n",
    "# if the API do not agree, throw error\n",
    "# all API length should all be 14 digits and end with 0000\n",
    "# also generate API_10 in this stage and make all API and API_10 int64 format \n",
    "print('before')\n",
    "print(\"min API in header           = \", len(str(np.min(Raw_Header.API))))\n",
    "print(\"min API in QMM              = \", len(str(np.min(Raw_Monthly_Production.API))))\n",
    "print(\"min API in Completion       = \", len(str(np.min(Raw_Completion.API))))\n",
    "# print(\"min API in Spacing          = \", len(str(np.min(Raw_Spacing.API))))\n",
    "print('*'*20)\n",
    "API_Digit_Difference = len(str(np.min(Raw_Header.API))) - len(str(np.min(Raw_Monthly_Production.API)))\n",
    "API_Digit_Difference_comp = len(str(np.min(Raw_Header.API))) - len(str(np.min(Raw_Completion.API)))\n",
    "\n",
    "\n",
    "# set_aside the small API and change that to 14 digits\n",
    "# this is assume there is only one API alternatives\n",
    "# then the max API length is not equal to min API length anymore\n",
    "# should thorw an error to prevent going forward\n",
    "\n",
    "# if there are 2 cases, where you have a mix of 10 digit and 12 digit API mixed, then\n",
    "# I need to manually do this job\n",
    "\n",
    "def Fill_14_Digits_API(table):\n",
    "    \n",
    "    # get the API < 14 digits\n",
    "    # 10 **13 is the minimal of 14 digit API\n",
    "    table_less = table[table.API < 10**13] \n",
    "    \n",
    "    # set aside the good API\n",
    "    table_set_aside = table[~table.API.isin(table_less.API)]\n",
    "    \n",
    "    # fill the less API with 0 towards 14 digit API\n",
    "    table_less['API'] = table_less['API']*(10**API_Digit_Difference)\n",
    "\n",
    "    # merge (concat) the less and set_aside\n",
    "    table_new = pd.concat([table_less,table_set_aside])\n",
    "    \n",
    "    return table_new\n",
    "\n",
    "def Fill_14_Digits_API_completion(table):\n",
    "    \n",
    "    # get the API < 14 digits\n",
    "    # 10 **13 is the minimal of 14 digit API\n",
    "    table_less = table[table.API < 10**13] \n",
    "    \n",
    "    # set aside the good API\n",
    "    table_set_aside = table[~table.API.isin(table_less.API)]\n",
    "    \n",
    "    # fill the less API with 0 towards 14 digit API\n",
    "    table_less['API'] = table_less['API']*(10**API_Digit_Difference_comp)\n",
    "\n",
    "    # merge (concat) the less and set_aside\n",
    "    table_new = pd.concat([table_less,table_set_aside])\n",
    "    \n",
    "    return table_new\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if BASIN != 'PER':\n",
    "Raw_Header = Fill_14_Digits_API(Raw_Header)\n",
    "Raw_Monthly_Production = Fill_14_Digits_API(Raw_Monthly_Production)\n",
    "Raw_Completion = Fill_14_Digits_API_completion(Raw_Completion)\n",
    "    # Raw_Spacing = Fill_14_Digits_API(Raw_Spacing)\n",
    "    \n",
    "\n",
    "# make sure all API are end with 0000\n",
    "Raw_Monthly_Production['API_10'] = Raw_Monthly_Production['API'] / 10000\n",
    "Raw_Monthly_Production['API_10'] = Raw_Monthly_Production['API_10'].astype(np.int64)\n",
    "Raw_Monthly_Production['API'] = Raw_Monthly_Production['API_10'] * 10000\n",
    "Raw_Monthly_Production['API'] = Raw_Monthly_Production['API'].astype(np.int64)\n",
    "\n",
    "Raw_Completion['API_10'] = Raw_Completion['API'] / 10000\n",
    "Raw_Completion['API_10'] = Raw_Completion['API_10'].astype(np.int64)\n",
    "Raw_Completion['API'] = Raw_Completion['API_10'] * 10000\n",
    "Raw_Completion['API'] = Raw_Completion['API'].astype(np.int64)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "print('after, check min and max, they have to be equal in order to move forward')\n",
    "print(\"min API in header           = \", len(str(np.min(Raw_Header.API)))   )\n",
    "print(\"min API in QMM              = \", len(str(np.min(Raw_Monthly_Production.API)))   )\n",
    "print(\"min API in Completion       = \", len(str(np.min(Raw_Completion.API)))   )\n",
    "# print(\"min API in Spacing          = \", len(str(np.min(Raw_Spacing.API)))   )\n",
    "\n",
    "print('*'*20)\n",
    "print(\"max API in header           = \", len(str(np.max(Raw_Header.API))))\n",
    "print(\"max API in QMM              = \", len(str(np.max(Raw_Monthly_Production.API)))   )\n",
    "print(\"max API in Completion       = \", len(str(np.max(Raw_Completion.API)))   )\n",
    "# print(\"max API in Spacing          = \", len(str(np.max(Raw_Spacing.API))))\n",
    "\n",
    "if len(str(np.min(Raw_Monthly_Production.API))) != len(str(np.max(Raw_Monthly_Production.API))):\n",
    "    raise ValueError('max and min API digits not equal, go check')\n",
    "\n",
    "if len(str(np.min(Raw_Monthly_Production.API))) != 14:\n",
    "    raise ValueError('min API digits not equal tp 14, go check')\n",
    "    \n",
    "if len(str(np.max(Raw_Monthly_Production.API))) != 14:\n",
    "    raise ValueError('min API digits not equal tp 14, go check')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function 3.1: calculate spacing in house (not use WellDB spacing anymore)\n",
    "\n",
    "def Get_Spacing(df_header):\n",
    "    \n",
    "    well_count = df_header.shape[0]\n",
    "    \n",
    "    # initialize the lists to be used\n",
    "    mid_latitude, mid_longitude = [], []\n",
    "    horizontal_spacing, total_spacing = [], []\n",
    "\n",
    "    # Calculate Mid-lateral Latitude and Longitude\n",
    "    for i in range(well_count):\n",
    "        mid_lat = (df_header.loc[i,'Latitude'] + df_header.loc[i,'BHLatitude']) / 2\n",
    "        mid_long = (df_header.loc[i,'Longitude'] + df_header.loc[i,'BHLongitude']) / 2\n",
    "        mid_latitude.append(mid_lat)\n",
    "        mid_longitude.append(mid_long)        \n",
    "\n",
    "    df_header['mid_latitude'] = mid_latitude\n",
    "    df_header['mid_longitude'] = mid_longitude\n",
    "\n",
    "    # now need to group the wells into 6 miles radius inside a loop\n",
    "    # target well i has an offset wells within 6 miles, which is 0.1 in lat and long\n",
    "\n",
    "    for i in range(well_count):\n",
    "        \n",
    "        target_mid_lat = df_header.loc[i,'mid_latitude']\n",
    "        target_mid_long = df_header.loc[i,'mid_longitude']\n",
    "\n",
    "        # first, get a group of wells within 0.1 lat/long to target well\n",
    "        df_offset = df_header[ (df_header['mid_latitude'] <=  target_mid_lat + 0.1) &\n",
    "                               (df_header['mid_latitude'] >=  target_mid_lat - 0.1) &\n",
    "                               (df_header['mid_longitude'] <=  target_mid_long + 0.1) &\n",
    "                               (df_header['mid_longitude'] >=  target_mid_long - 0.1) \n",
    "                             ]\n",
    "\n",
    "        df_offset = df_offset.reset_index(drop=True)\n",
    "        \n",
    "        # set the maximum cap of horizontal spacing to be 5280\n",
    "        min_distance = 5280\n",
    "        total_distance = 5280\n",
    "        \n",
    "        # calculate the distance between mid point to the offset wells\n",
    "        for j in range(df_offset.shape[0]):\n",
    "            distance_to_target = get_distance(\n",
    "                target_mid_lat,\n",
    "                target_mid_long,\n",
    "                df_offset.loc[j,'mid_latitude'],\n",
    "                df_offset.loc[j,'mid_longitude'],        \n",
    "            )   \n",
    "        \n",
    "        # if distanc_to_target is less than 5280, and it is > 0, so update the min distance\n",
    "        # min_distance should be the horizontal spacing\n",
    "            if (distance_to_target < min_distance) & (distance_to_target > 0):\n",
    "                min_distance = distance_to_target\n",
    "                \n",
    "                # get the horizontal distance then get the total distance \n",
    "                # adding vertical portion\n",
    "                \n",
    "                # fillna of TVD\n",
    "                df_header['TrueVerticalDepth'].fillna((df_header['TrueVerticalDepth'].mean()), inplace=True)\n",
    "                \n",
    "                # get the total distance using horizontal and vertical distance\n",
    "                vertical_difference = df_header.loc[i,'TrueVerticalDepth'] - df_header.loc[j,'TrueVerticalDepth']\n",
    "                total_distance = np.sqrt(vertical_difference**2 + min_distance**2)\n",
    "                \n",
    "        min_distance = int(min_distance)\n",
    "#         print(min_distance)       \n",
    "        total_distance = int(min(total_distance,5280))\n",
    "        \n",
    "        horizontal_spacing.append(min_distance)\n",
    "        total_spacing.append(total_distance)\n",
    "    \n",
    "    # finish and output\n",
    "    df_header['Spacing'] = horizontal_spacing\n",
    "    df_header['total_spacing'] = total_spacing\n",
    "    \n",
    "    df_header['Spacing'][df_header['Spacing'] == 0] = 100\n",
    "    df_header['total_spacing'][df_header['total_spacing'] == 0] = 100\n",
    "    \n",
    "    return df_header\n",
    "        \n",
    "Raw_Header = Get_Spacing(Raw_Header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# function 3.2: merge completion and spacing to the raw header\n",
    "\n",
    "\n",
    "\n",
    "# only 1 line of code to get the completion data\n",
    "# Extract total proppant column from completion table\n",
    "df_completion = Raw_Completion[['API','TotalProppantMass']]\n",
    "df_completion = df_completion.sort_values(by=['API', 'TotalProppantMass'])\n",
    "df_completion = df_completion.drop_duplicates(subset='API', keep=\"last\")\n",
    "df_completion = df_completion[df_completion.API.isin(Raw_Header.API)]\n",
    "# df_completion\n",
    "\n",
    "# Sort the header file\n",
    "\n",
    "\n",
    "Raw_Header = Raw_Header[Raw_Header['API'] > 0]\n",
    "Raw_Header = Raw_Header.sort_values(['API'], ascending = True)\n",
    "Raw_Header.reset_index(drop = True,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Merge completion table and header table, with left join\n",
    "df_header = pd.merge(Raw_Header, df_completion, how='left', on=['API'])\n",
    "df_header = df_header.drop_duplicates()\n",
    "\n",
    "# get the frac inteensity\n",
    "df_header['FracIntensity'] = df_header['TotalProppantMass'] / df_header['LateralLength']\n",
    "\n",
    "# fill the frac intensity of inf and na with 0\n",
    "df_header['FracIntensity'] = df_header['FracIntensity'].replace(-np.inf, np.nan)           \n",
    "df_header['FracIntensity'].fillna(0,inplace=True)\n",
    "\n",
    "# limit the frac intensity to be <= 10000\n",
    "df_header['FracIntensity'][df_header['FracIntensity'] > 10000] = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function 3.3: fill all the blank lease ID with lease numbers\n",
    "# make sure all lease ID has values \n",
    "# this is for Texas and Luisianna only \n",
    "\n",
    "if BASIN in ['EF', 'PER', 'HAY']:\n",
    "#     df_header = Fill_LeaseID(df_header)\n",
    "    df_header.LeaseId = np.where(df_header.LeaseId.isnull(), df_header.Lease, df_header.LeaseId)\n",
    "            \n",
    "# df_header.LeaseId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for Permian only\n",
    "\n",
    "if BASIN in ['PER']:\n",
    "    \n",
    "    BS_List = ['ABO', 'ABO NORTH', 'BONE SPRING', 'BONE SPRING, SOUTHWEST',\n",
    "       'BONE SPRING, SOUTHWWEST', 'DELAWARE', 'DELAWARE/BS',\n",
    "       'FORTY NINER RIDGE BONE SPRING WEST', 'GLORIETA',\n",
    "       'LOWER BONE SPRING', 'LOWER BS', 'LWR BONE SPRIN', 'SA-YESO',\n",
    "           'UPPER BONE SPR', 'UPPER BONE SPRING SHALE','FORTY NINER RIDGE BS WEST','UPPER BS SHALE','UPPER BSS',\n",
    "       'UPPER BONE SPRINGS', 'UPR BONE SPRIN',  'YESO', 'BS'\n",
    "          ]\n",
    "           \n",
    "           \n",
    "    SP_List = [ 'SPRABERRY', 'SP'        ]\n",
    "    WC_List = [ 'WLFCMPPENN CONS',\n",
    "           'WOLFBONE', 'WOLFCAMP', 'WOLFELLEN CONS','DEVONIAN','CONSOLIDATED','WC'       ]\n",
    "    \n",
    "    \n",
    "    for index, row in df_header.iterrows():\n",
    "    \n",
    "        if df_header.loc[index,'PrimaryFormation'] in BS_List:\n",
    "            df_header.loc[index,'PrimaryFormation'] = 'BS'\n",
    "\n",
    "        if df_header.loc[index,'PrimaryFormation'] in SP_List:\n",
    "            df_header.loc[index,'PrimaryFormation'] = 'SP'      \n",
    "\n",
    "        if df_header.loc[index,'PrimaryFormation'] in WC_List:\n",
    "            df_header.loc[index,'PrimaryFormation'] = 'WC'\n",
    "    \n",
    "    print(df_header.PrimaryFormation.unique())\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw_Monthly_Production['WellWater'] = 0\n",
    "# Raw_Monthly_Production['TotalWater'] = 0\n",
    "# Raw_Monthly_Production['TotalGas'] = 0\n",
    "# Raw_Monthly_Production['MonthNumber'] = 0\n",
    "# Raw_Monthly_Production['TotalOil'] = 0\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw file: \n",
      "(2576773, 11)\n",
      "27634\n",
      "(27641, 37)\n",
      "27641\n",
      "********************\n",
      "processed file: \n",
      "(2574612, 10)\n",
      "27608\n",
      "(27608, 39)\n",
      "27608\n"
     ]
    }
   ],
   "source": [
    "# function 4.1: remember to assign all production days to 30.4375\n",
    "\n",
    "Raw_Monthly_Production['Days'] = 30.4375\n",
    "\n",
    "# function 4.2: initialize Raw Monthly Production Data\n",
    "\n",
    "QMM_needed_columns = ['API','ReportDate','Days','TotalOil','WellOil',\n",
    "                     'TotalGas','WellGas','TotalWater','WellWater','MonthNumber']\n",
    "\n",
    "df_raw_production = Raw_Monthly_Production[QMM_needed_columns]\n",
    "df_raw_production['ReportDate'] = pd.to_datetime(df_raw_production['ReportDate'])\n",
    "\n",
    "# make sure the headers and raw productions are matching\n",
    "df_raw_production = df_raw_production[df_raw_production.API.isin(df_header.API)] \n",
    "df_header = df_header[df_header.API.isin(df_raw_production.API)] \n",
    "\n",
    "# sorting\n",
    "df_raw_production = df_raw_production.sort_values([\"API\", \"ReportDate\"], ascending = (True, True)) \n",
    "df_raw_production = df_raw_production.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print('raw file: ')\n",
    "print(Raw_Monthly_Production.shape)\n",
    "print(Raw_Monthly_Production.API.nunique())\n",
    "print(Raw_Header.shape)\n",
    "print(Raw_Header.API.nunique())\n",
    "\n",
    "print('*'*20)\n",
    "print('processed file: ')\n",
    "print(df_raw_production.shape)\n",
    "print(df_raw_production.API.nunique())\n",
    "print(df_header.shape)\n",
    "print(df_header.API.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2574612, 10)\n",
      "27608\n",
      "(27608, 39)\n",
      "27608\n"
     ]
    }
   ],
   "source": [
    "# function 5: make sure monthly production oil, gas and water has no null or negative values\n",
    "\n",
    "def Fix_Null_Negative(df_raw_production):\n",
    "    \n",
    "    df_raw_production['WellOil'].fillna(0,inplace=True)\n",
    "    df_raw_production['WellGas'].fillna(0,inplace=True)\n",
    "    df_raw_production['WellWater'].fillna(0,inplace=True)\n",
    "    \n",
    "    \n",
    "    df_raw_production['WellOil'][df_raw_production['WellOil'] < 0] = 0\n",
    "    df_raw_production['WellGas'][df_raw_production['WellGas'] < 0] = 0\n",
    "    df_raw_production['WellWater'][df_raw_production['WellWater'] < 0] = 0\n",
    "    \n",
    "    return df_raw_production\n",
    "    \n",
    "\n",
    "    \n",
    "df_raw_production = Fix_Null_Negative(df_raw_production)   \n",
    "    \n",
    "print(df_raw_production.shape)\n",
    "print(df_raw_production.API.nunique())\n",
    "\n",
    "print(df_header.shape)\n",
    "print(df_header.API.nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_production:\n",
      "(2513017, 10)\n",
      "27608\n",
      "********************\n",
      "header:\n",
      "(27608, 39)\n",
      "27608\n"
     ]
    }
   ],
   "source": [
    "# function 1: remove double entry\n",
    "\n",
    "def Remove_Double_Entry(df_raw_production):\n",
    "    \n",
    "    # change the date format to be start of the month\n",
    "    df_raw_production['ReportDate'] = df_raw_production['ReportDate'].values.astype('datetime64[M]')\n",
    "\n",
    "    # drop absolutely same records first\n",
    "    df_trim_production = df_raw_production.drop_duplicates(subset=['API', 'ReportDate','Days',\n",
    "                                                                'WellOil','WellGas','WellWater'], keep='first')\n",
    "    \n",
    "    # drop duplicates where same report date has multiple records\n",
    "    df_clean_production = df_trim_production.drop_duplicates(subset= ['API','ReportDate'])\n",
    "    \n",
    "    # sum the well oil, gas, water records, this is what we agreed on \n",
    "    df_clean_production.WellOil = np.asarray(df_trim_production.groupby(['API','ReportDate'])['WellOil'].sum())\n",
    "    df_clean_production.WellGas = np.asarray(df_trim_production.groupby(['API','ReportDate'])['WellGas'].sum())\n",
    "    df_clean_production.WellWater = np.asarray(df_trim_production.groupby(['API','ReportDate'])['WellWater'].sum())\n",
    "\n",
    "\n",
    "    return df_clean_production\n",
    "    \n",
    "df_clean_production = Remove_Double_Entry(df_raw_production)\n",
    "df_header = df_header[df_header.API.isin(df_clean_production['API'])]\n",
    "df_header.drop_duplicates(subset=['API'],inplace=True)\n",
    "\n",
    "# check\n",
    "print('clean_production:')\n",
    "print(df_clean_production.shape)\n",
    "print(df_clean_production.API.nunique())\n",
    "print('*'*20)\n",
    "print('header:')\n",
    "print(df_header.shape)\n",
    "print(df_header.API.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_production:\n",
      "(2512188, 10)\n",
      "27600\n",
      "********************\n",
      "header:\n",
      "(27600, 43)\n",
      "27600\n"
     ]
    }
   ],
   "source": [
    "# function 2.1 remove salt water disposial well\n",
    "# it is defined as cum oil + cum gas == 0 but cum water > 0\n",
    "\n",
    "df_header['Cum_Oil'] = np.asarray(df_clean_production.groupby('API')['WellOil'].sum())\n",
    "df_header['Cum_Gas'] = np.asarray(df_clean_production.groupby('API')['WellGas'].sum())\n",
    "df_header['Cum_Water'] = np.asarray(df_clean_production.groupby('API')['WellWater'].sum())\n",
    "df_header['Cum_Hydrocarbon'] = df_header['Cum_Oil'] + df_header['Cum_Gas'] \n",
    "df_header = df_header[df_header['Cum_Hydrocarbon'] > 0]\n",
    "df_clean_production = df_clean_production[df_clean_production.API.isin(df_header.API)] \n",
    "\n",
    "\n",
    "# check\n",
    "print('clean_production:')\n",
    "print(df_clean_production.shape)\n",
    "print(df_clean_production.API.nunique())\n",
    "print('*'*20)\n",
    "print('header:')\n",
    "print(df_header.shape)\n",
    "print(df_header.API.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_production:\n",
      "(2119910, 10)\n",
      "27600\n",
      "********************\n",
      "header:\n",
      "(27600, 47)\n",
      "27600\n"
     ]
    }
   ],
   "source": [
    "# function 2.2: remove (Oil + Gas + Water <= 0)\n",
    "# this is aligned with we agreed with Varya\n",
    "# shut-in defination is now where oil + gas + water <= 0\n",
    "\n",
    "df_shutin_months = df_clean_production[(df_clean_production['WellOil'] <= 0) &\n",
    "                                      (df_clean_production['WellGas'] <= 0 ) &\n",
    "                                      (df_clean_production['WellWater'] <= 0 ) \n",
    "                                     ]\n",
    "\n",
    "to_drop_index = df_shutin_months.index.to_list()\n",
    "\n",
    "df_clean_production.drop(to_drop_index,inplace=True)\n",
    "\n",
    "# function 2.1: filter dummy, since we change how we handle shut-in and Days\n",
    "# this is not need any more\n",
    "df_header['Filter1'] = 0\n",
    "df_header['Filter2'] = 0\n",
    "df_header['Filter3'] = 0\n",
    "df_header['Filter4'] = 0\n",
    "\n",
    "\n",
    "\n",
    "print('clean_production:')\n",
    "print(df_clean_production.shape)\n",
    "print(df_clean_production.API.nunique())\n",
    "print('*'*20)\n",
    "print('header:')\n",
    "print(df_header.shape)\n",
    "print(df_header.API.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2119910, 14)\n",
      "27600\n",
      "(27600, 47)\n",
      "27600\n"
     ]
    }
   ],
   "source": [
    "# function 3: get daily productions\n",
    "\n",
    "df_final_production = df_clean_production\n",
    "df_final_production = df_final_production[df_final_production.API.isin(df_header.API)]\n",
    "df_header = df_header[df_header.API.isin(df_final_production.API)]\n",
    "df_header.drop_duplicates(subset='API',inplace=True)\n",
    "\n",
    "df_final_production.drop('MonthNumber', axis=1, inplace=True)\n",
    "df_final_production['ReportDate'] = pd.to_datetime(df_final_production['ReportDate'])\n",
    "\n",
    "\n",
    "# Permian origional MonthNumber column is bad, so create new MonthNumber as Rank to fix the bug\n",
    "df_final_production = df_final_production.drop_duplicates(subset= ['API','ReportDate'])\n",
    "df_final_production = df_final_production.sort_values([\"API\", \"ReportDate\"], ascending = (True, True))\n",
    "df_final_production['MonthNumber'] = df_final_production.groupby('API')['ReportDate'].rank(ascending=True)\n",
    "df_final_production[\"MonthNumber\"] = df_final_production[\"MonthNumber\"].astype(np.int64)\n",
    "\n",
    "\n",
    "df_final_production['Time'] = df_final_production['MonthNumber'] / 12\n",
    "df_final_production['BOPD_Oil'] = df_final_production['WellOil'] / df_final_production['Days']\n",
    "df_final_production['BOPD_Gas'] = df_final_production['WellGas'] / df_final_production['Days']\n",
    "df_final_production['BOPD_Water'] = df_final_production['WellWater'] / df_final_production['Days']\n",
    "\n",
    "\n",
    "\n",
    "print(df_final_production.shape)\n",
    "print(df_final_production.API.nunique())\n",
    "print(df_header.shape)\n",
    "print(df_header.API.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function 4: get Jan 2018, 2015, 2012 positions for historical study\n",
    "\n",
    "### To add the Jan 2018 position for each well to file 1\n",
    "\n",
    "Jan_2018_Position = []\n",
    "API_List = df_header.API\n",
    "\n",
    "for i in API_List:\n",
    "    API_Oil = df_final_production[df_final_production['API'] == i]\n",
    "#     print(i)\n",
    "    s = (API_Oil.loc[API_Oil.index,'ReportDate'] >= np.datetime64('2018-01-01'))\n",
    "    if not any(s.values) == False:\n",
    "      \n",
    "        t = np.where(s.values == True)\n",
    "        if len(t) > 0:\n",
    "            Jan_2018_Position.append(t[0][0] + 1)\n",
    "    \n",
    "    else:\n",
    "        Jan_2018_Position.append(-1)\n",
    "\n",
    "df_header['Jan_2018_Position'] = Jan_2018_Position\n",
    "\n",
    "\n",
    "### To add the Jan 2015 position for each well to file 1\n",
    "\n",
    "Jan_2015_Position = []\n",
    "API_List = df_header.API\n",
    "\n",
    "for i in API_List:\n",
    "    API_Oil = df_final_production[df_final_production['API'] == i]\n",
    "#     print(i)\n",
    "    s = (API_Oil.loc[API_Oil.index,'ReportDate'] >= np.datetime64('2015-01-01'))\n",
    "    if not any(s.values) == False:\n",
    "      \n",
    "        t = np.where(s.values == True)\n",
    "        if len(t) > 0:\n",
    "            Jan_2015_Position.append(t[0][0] + 1)\n",
    "    \n",
    "    else:\n",
    "        Jan_2015_Position.append(-1)\n",
    "\n",
    "df_header['Jan_2015_Position'] = Jan_2015_Position\n",
    "\n",
    "\n",
    "\n",
    "### To add the Jan 2012 position for each well to file 1\n",
    "\n",
    "Jan_2012_Position = []\n",
    "API_List = df_header.API\n",
    "\n",
    "for i in API_List:\n",
    "    API_Oil = df_final_production[df_final_production['API'] == i]\n",
    "#     print(i)\n",
    "    s = (API_Oil.loc[API_Oil.index,'ReportDate'] >= np.datetime64('2012-01-01'))\n",
    "    if not any(s.values) == False:\n",
    "      \n",
    "        t = np.where(s.values == True)\n",
    "        if len(t) > 0:\n",
    "            Jan_2012_Position.append(t[0][0] + 1)\n",
    "    \n",
    "    else:\n",
    "        Jan_2012_Position.append(-1)\n",
    "\n",
    "df_header['Jan_2012_Position'] = Jan_2012_Position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function 5: generate df_QMM file\n",
    "\n",
    "\n",
    "df_QMM = df_final_production[['API','ReportDate','BOPD_Oil','BOPD_Gas',\n",
    "                              'BOPD_Water','Days','WellOil','WellGas','WellWater']]\n",
    "df_QMM['ReportDate'] = pd.to_datetime(df_QMM['ReportDate'])\n",
    "\n",
    "\n",
    "# Permian origional MonthNumber column is bad, so create new MonthNumber as Rank to fix the bug\n",
    "\n",
    "df_QMM['Instance'] = df_QMM.groupby('API')['ReportDate'].rank(ascending=True)\n",
    "df_QMM[\"Instance\"] = df_QMM[\"Instance\"].astype(np.int64)\n",
    "\n",
    "\n",
    "df_QMM.rename({'BOPD_Oil': 'ActualProdOil', \n",
    "                          'BOPD_Gas': 'ActualProdGas',\n",
    "                          'BOPD_Water': 'ActualProdWater',}, axis=1, inplace=True)\n",
    "\n",
    "df_QMM.drop_duplicates(subset= ['API','ReportDate'], inplace= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function 6: generate File2, File3 and File4\n",
    "\n",
    "if BASIN != 'EF' and BASIN != 'MC':\n",
    "\n",
    "    df_oil = df_QMM.pivot(index='Instance',columns='API',values='ActualProdOil') \n",
    "    df_gas = df_QMM.pivot(index='Instance',columns='API',values='ActualProdGas') \n",
    "    df_water = df_QMM.pivot(index='Instance',columns='API',values='ActualProdWater') \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    df_oil['Time'] = df_oil.index / 12\n",
    "    df_oil = df_oil.set_index('Time')\n",
    "    # df_oil.drop('Instance',axis=1,inplace=True)\n",
    "    df_oil.to_csv('df_oil_File2.csv')\n",
    "\n",
    "    df_gas['Time'] = df_gas.index / 12\n",
    "    df_gas = df_gas.set_index('Time')\n",
    "    # df_gas.drop('Instance',axis=1,inplace=True)\n",
    "    df_gas.to_csv('df_gas_File3.csv')\n",
    "\n",
    "    df_water['Time'] = df_water.index / 12\n",
    "    df_water = df_water.set_index('Time')\n",
    "    # df_gas.drop('Instance',axis=1,inplace=True)\n",
    "    df_water.to_csv('df_water_File4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function 7.1:  First/Last Production Date\n",
    "\n",
    "df_header.drop(['FirstProdDate', 'LastProdDate'], axis=1, inplace=True)\n",
    "df_QMM = df_QMM[df_QMM.API.isin(df_header.API)]\n",
    "df_header = df_header[df_header.API.isin(df_QMM.API)]\n",
    "df_QMM['ReportDate'] = pd.to_datetime(df_QMM['ReportDate'])\n",
    "\n",
    "df_QMM['ReportDate'] = df_QMM['ReportDate'].values.astype('datetime64[M]')\n",
    "df_QMM.drop_duplicates(subset= ['API','ReportDate'], inplace= True)\n",
    "\n",
    "\n",
    "df_FirstProdDate = df_QMM.loc[df_QMM.groupby('API')['ReportDate'].idxmin()]\n",
    "df_LastProdDate = df_QMM.loc[df_QMM.groupby('API')['ReportDate'].idxmax()]\n",
    "\n",
    "# df_QMM[df_QMM['API'] == 42389371400000]\n",
    "# df_FirstProdDate[df_FirstProdDate['API'] == 30015384320000]\n",
    "\n",
    "df_FirstProdDate_simple = df_FirstProdDate[['API','ReportDate']]\n",
    "df_FirstProdDate_simple.rename(columns={'ReportDate':'FirstProdDate'}, inplace=True)\n",
    "\n",
    "df_LastProdDate_simple = df_LastProdDate[['API','ReportDate']]\n",
    "df_LastProdDate_simple.rename(columns={'ReportDate':'LastProdDate'}, inplace=True)\n",
    "\n",
    "df_header = pd.merge(df_header, df_FirstProdDate_simple, on='API')\n",
    "df_header = pd.merge(df_header, df_LastProdDate_simple, on='API')\n",
    "\n",
    "# df_header['FirstProdDate'] = pd.to_datetime(df_header['FirstProdDate'])\n",
    "df_header['FirstProdDate'] = df_header['FirstProdDate'].dt.strftime('%m/%d/%Y')\n",
    "# df_header['LastProdDate'] = pd.to_datetime(df_header['LastProdDate'])\n",
    "df_header['LastProdDate'] = df_header['LastProdDate'].dt.strftime('%m/%d/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function 7.2: calcualte GOR, TotalMonthsHistory\n",
    "\n",
    "df_header['IP90Oil'] = df_QMM.groupby('API')['WellOil'].apply(lambda x: x.head(3).sum()).values\n",
    "df_header['IP180Oil'] = df_QMM.groupby('API')['WellOil'].apply(lambda x: x.head(6).sum()).values\n",
    "df_header['IP360Oil'] = df_QMM.groupby('API')['WellOil'].apply(lambda x: x.head(12).sum()).values\n",
    "df_header['IP90Gas'] = df_QMM.groupby('API')['WellGas'].apply(lambda x: x.head(3).sum()).values\n",
    "df_header['IP180Gas'] = df_QMM.groupby('API')['WellGas'].apply(lambda x: x.head(6).sum()).values\n",
    "df_header['IP360Gas'] = df_QMM.groupby('API')['WellGas'].apply(lambda x: x.head(12).sum()).values\n",
    "df_header['IP90Water'] = df_QMM.groupby('API')['WellWater'].apply(lambda x: x.head(3).sum()).values\n",
    "df_header['IP180Water'] = df_QMM.groupby('API')['WellWater'].apply(lambda x: x.head(6).sum()).values\n",
    "df_header['IP360Water'] = df_QMM.groupby('API')['WellWater'].apply(lambda x: x.head(12).sum()).values\n",
    "df_header['90DayGOR'] = 1000 * df_header['IP90Gas'] / df_header['IP90Oil']\n",
    "df_header['180DayGOR'] = 1000 * df_header['IP180Gas'] / df_header['IP180Oil']\n",
    "df_header['360DayGOR'] = 1000 * df_header['IP360Gas'] / df_header['IP360Oil']\n",
    "df_header['TotalMonthsHistory'] = df_QMM.groupby('API')['Instance'].max().values\n",
    "# df_header['Cum_Oil'] = np.asarray(df_QMM.groupby('API')['WellOil'].sum())\n",
    "# df_header['Cum_Gas'] = np.asarray(df_QMM.groupby('API')['WellGas'].sum())\n",
    "# df_header['Cum_Water'] = np.asarray(df_QMM.groupby('API')['WellWater'].sum())\n",
    "\n",
    "df_header['Months_After_Jan_2018'] = df_header['TotalMonthsHistory'] - df_header['Jan_2018_Position'] + 1\n",
    "df_header['Months_After_Jan_2015'] = df_header['TotalMonthsHistory'] - df_header['Jan_2015_Position'] + 1\n",
    "df_header['Months_After_Jan_2012'] = df_header['TotalMonthsHistory'] - df_header['Jan_2012_Position'] + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function 8: export all results\n",
    "\n",
    "df_header['FirstProdDate'] = pd.to_datetime(df_header['FirstProdDate'])\n",
    "df_header['FirstProdDate'] = df_header['FirstProdDate'].dt.strftime('%m/%d/%Y')\n",
    "df_header['LastProdDate'] = pd.to_datetime(df_header['LastProdDate'])\n",
    "df_header['LastProdDate'] = df_header['LastProdDate'].dt.strftime('%m/%d/%Y')\n",
    "df_QMM['ReportDate'] = pd.to_datetime(df_QMM['ReportDate'])\n",
    "df_QMM['ReportDate'] = df_QMM['ReportDate'].dt.strftime('%m/%d/%Y')\n",
    "\n",
    "if BASIN != 'EF' and BASIN != 'MC'  :\n",
    "    df_header.to_csv('df_header.csv',index=False)\n",
    "    df_QMM.to_csv('df_QMM.csv',index=False)\n",
    "\n",
    "if BASIN == 'EF':\n",
    "    df_header.to_csv('df_header_ALL_EF.csv',index=False)\n",
    "    df_QMM.to_csv('df_QMM_ALL_EF.csv',index=False)\n",
    "    \n",
    "if BASIN == 'MC':\n",
    "    df_header.to_csv('df_header_ALL_MC.csv',index=False)\n",
    "    df_QMM.to_csv('df_QMM_ALL_MC.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
